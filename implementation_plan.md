# Implementation Plan - AeroCleanse ETL Pipeline

## Goal Description
Develop a localized Data Engineering portfolio project that simulates a DoD Maintenance, Repair, and Overhaul (MRO) data pipeline. The system will ingest raw, unstructured maintenance logs, clean and standardize them using Python and Regex, and load them into a relational database for analysis.
**Target Role:** Data Engineer (LDSS/DoD support)
**Skills Demonstrated:** Python, Regex Data Cleansing, SQL, ETL Pipeline Design, Data Modeling.

## User Review Required
> [!NOTE]
> This plan uses local file directories to simulate AWS S3 buckets to keep the project self-contained and free of cloud costs.

## Proposed Architecture
- **Source:** Synthetic CSV/JSON files generated by a script (simulating field reports).
- **Ingestion:** A "landing zone" directory watched by the pipeline.
- **Processing:** Python script using Pandas and standard `re` library.
- **Storage:** SQLite database (Simulating a Data Warehouse).

## Proposed Changes / File Structure

### Project Root: `AeroCleanse_ETL/`

#### [NEW] [src/data_generator.py](file:///Users/jonathanekowapprey/Desktop/SoftwareProjects/portfolio/AeroCleanse_ETL/src/data_generator.py)
*   **Purpose:** Generate "messy" data to prove the cleaner works.
*   **Logic:**
    *   Use `faker` to generate random dates and names.
    *   Randomly insert "dirty" data: dates in `MM-DD-YYYY` vs `YYYY/MM/DD` format, null values in critical fields.
    *   Embed error codes (e.g., `ERR-202X-Alpha`) inside long text descriptions to require Regex extraction.

#### [NEW] [src/db_setup.py](file:///Users/jonathanekowapprey/Desktop/SoftwareProjects/portfolio/AeroCleanse_ETL/src/db_setup.py)
*   **Purpose:** Initialize the SQLite database.
*   **Schema:**
    *   `maintenance_logs`: `id` (PK, Auto), `aircraft_id` (Text), `event_date` (Date), `error_code` (Text), `description` (Text), `technician` (Text).

#### [NEW] [src/etl_pipeline.py](file:///Users/jonathanekowapprey/Desktop/SoftwareProjects/portfolio/AeroCleanse_ETL/src/etl_pipeline.py)
*   **Purpose:** The core Logic.
*   **Functions:**
    *   `extract_data()`: Read all files in `data/raw`.
    *   `transform_data(df)`:
        *   **Regex Step:** `df['error_code'] = df['description'].apply(lambda x: re.search(r'ERR-\d{4}-\w+', x).group(0) if ...)`
        *   **Date Step:** Convert all date formats to `YYYY-MM-DD`.
        *   **Cleaning:** Drop rows where `aircraft_id` is missing.
    *   `load_data(df)`: Append cleaned dataframe to the SQLite `maintenance_logs` table.

## Verification Plan

### Automated Tests
*   Run the generator: `python src/data_generator.py --count 50`
*   Run the pipeline: `python src/etl_pipeline.py`
*   Verify output:
    ```bash
    sqlite3 maintenance.db "SELECT count(*) FROM maintenance_logs;"
    sqlite3 maintenance.db "SELECT * FROM maintenance_logs WHERE error_code IS NOT NULL LIMIT 5;"
    ```

### Manual Verification
*   Inspect `data/processed` to ensure raw files were moved or archived after processing.
*   Check the console logs for "dirty data" rejection statistics.
